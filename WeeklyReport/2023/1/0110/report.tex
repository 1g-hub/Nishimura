
%\documentstyle[epsf,twocolumn]{jarticle}       %LaTeX2.09仕様
%\documentclass[twocolumn]{jarticle}     %pLaTeX2e仕様
\documentclass{jarticle}     %pLaTeX2e仕様

%一枚組だったら[twocolumn]関係のとこ消す

\setlength{\topmargin}{-45pt}
%\setlength{\oddsidemargin}{0cm} 
\setlength{\oddsidemargin}{-7.5mm}
%\setlength{\evensidemargin}{0cm} 
\setlength{\textheight}{24.1cm}
%setlength{\textheight}{25cm} 
\setlength{\textwidth}{17.4cm}
%\setlength{\textwidth}{172mm} 
\setlength{\columnsep}{11mm}

\kanjiskip=.07zw plus.5pt minus.5pt

\usepackage{graphicx}
\usepackage[dvipdfmx]{color}
\usepackage{subcaption}
\usepackage{enumerate}
\usepackage{comment}
\usepackage{url}
\usepackage{multirow}
\usepackage{diagbox}
\usepackage{amsmath,amssymb}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage{float}
\usepackage{algorithmic}
\usepackage{algorithm}



\begin{document}
  \noindent
  \onecolumn
  \hspace{1em}

  \today
  \hfill
  \ \ B3 西村昭賢 

  \vspace{2mm}
  \hrule
  \begin{center}
  {\Large \bf 進捗報告}
  \end{center}
  \hrule
  \vspace{3mm}


\section{やったこと}
\begin{itemize}
  \item 「何もしない」を行動空間に含めた際に「何もしない」が損であるかどうか学習できているかの確認
  \item ランダムに対戦するプレイヤー同士の対戦環境におけるバランス調整
  \item 構築環境への DQN の適用実験
\end{itemize}

\section{「何もしない」を行動空間に含めた場合の学習}
コストや, プレイヤーの HP を追加した環境において, 行動空間に「何もしない」という行動を追加した場合には結果としてルールベースの相手に対して高い勝率を残すことができなかった. 
\subsection{実験}
そこで表 \ref{table:action2} に示す行動空間に従い, 学習序盤と学習後期として 30000 ステップ, 3000000 ステップとステップ数 DQN で実験し 10000 回対戦を実行し勝率とエージェントが選択する行動を観測し「何もしない」という行動が損であるかどうか確認した. 


\begin{table}[ht]
  \centering
  \caption{実験で定義した行動空間 (太字は「何もしない」に対応する行動)}
  \vspace{-0.3cm}
  \label{table:action2}
  \begin{tabular}{|c|c|}
  \hline
  行動説明                          & 次元数        \\ \hline
  手札 1 $\sim$ 9 を自盤面に出す             & 9          \\ \hline
  \textbf{手札 1 $\sim$ 9 を自盤面に出さない} & \textbf{9} \\ \hline
  自盤面 1 が敵盤面 1 $\sim$ 5 に攻撃 or 敵プレイヤーに攻撃    &  6          \\ \hline
  自盤面 2 が敵盤面 1 $\sim$ 5 に攻撃 or 敵プレイヤーに攻撃    & 6   \\ \hline
  自盤面 3 が敵盤面 1 $\sim$ 5 に攻撃 or 敵プレイヤーに攻撃    & 6 \\ \hline
  自盤面 4 が敵盤面 1 $\sim$ 5 に攻撃 or 敵プレイヤーに攻撃    & 6 \\ \hline
  自盤面 5 が敵盤面 1 $\sim$ 5 に攻撃 or 敵プレイヤーに攻撃    & 6 \\ \hline
  \end{tabular}
  \end{table}

  
      
  \subsection{結果}

  表 \ref{table:resultwinrate} に勝率を示す. 3000000 ステップ学習したエージェントのほうが高い勝率を残している. また表 \ref{table:30000action} , \ref{table:3000000action} にエージェントがとった行動の総数を示す.
  \begin{table}[ht]
    \centering
    \caption{勝率}
    \vspace{-0.3cm}
    \label{table:resultwinrate}
    \scalebox{1.0}[1.0]{
      \begin{tabular}{|c|c|}
        \hline
        ステップ数 & 勝率 \\ \hline
        30000 & 0.1169 \\ \hline
        3000000 & 0.3656 \\ \hline
        \end{tabular}
    }
    \end{table}
  
    \begin{table}[ht]
      \centering
      \caption{30000 ステップ学習したエージェントの 10000 回の対戦実行における行動総数上位 5 つ (盤面のカードの攻撃については実験に関係ないので除外)}
      \vspace{-0.3cm}
      \label{table:30000action}
      \scalebox{1.0}[1.0]{
        \begin{tabular}{|c|c|}
          \hline
          行動説明 & 総数 \\ \hline
          手札 1 を盤面に出さない & 92698 \\ \hline
          手札 2 を盤面に出さない & 87254 \\ \hline
          手札 3 を盤面に出さない & 81907 \\ \hline
          手札 4 を盤面に出さない & 61406 \\ \hline
          手札 5 を盤面に出さない & 37569 \\ \hline
          \end{tabular}
      }
      \end{table}
    
      \begin{table}[ht]
        \centering
        \caption{3000000 ステップ学習したエージェントの 10000 回の対戦実行における行動総数上位 5 つ (盤面のカードの攻撃については実験に関係ないので除外)}
        \vspace{-0.3cm}
        \label{table:3000000action}
        \scalebox{1.0}[1.0]{
          \begin{tabular}{|c|c|}
            \hline
            行動説明 & 総数 \\ \hline
            手札 1 を盤面に出さない & 109802 \\ \hline
            手札 2 を盤面に出さない & 76544 \\ \hline
            手札 2 を盤面に出す & 72931 \\ \hline
            手札 3 を盤面に出さない & 62698 \\ \hline
            手札 1 を盤面に出す & 58919 \\ \hline
            \end{tabular}
        }
        \end{table}

  \subsection{考察}
  学習が進むにつれ, 手札を盤面に出す行動の回数が増えていることが分かる.
  しかし, 依然として「盤面に出さない」という選択肢も選択されている. これは行動空間の与え方によるエージェントのターンエンドに原因があると考えられる. エージェントは表 \ref{table:action2} に与える行動空間に沿って, 盤面と手札に存在する全てのカードについて何かしら行動をしていたらターンエンドとしている. この方法で学習が上手く行ったカードのコストが無い以前の環境は, 手札について考えると「何もしない」すなわち盤面に出さないという行動を選択することはあえてカードを出さないという戦略的な意味を持った行動となる.\par
  しかし, 今回のカードのコストが存在するカードでは, 「何もしない」という行動はあえてカードを出さないといった戦略的な意味を持った行動だけでなく, プレイヤーのコスト的にどうしても盤面に出せないからターンエンドを迎えるために選択するといった戦略的には意味のない行動にもなりうる. そのため学習の際に「何もしない」という行動が過大評価されて上手く学習が進まないと考えられる.

\section{ランダムに行動するプレイヤー同士の対戦におけるバランス調整}

ランダムに対戦する動詞の勝率は先攻, 後攻の勝率は約 50 \% となった.
\subsection{実験 1 }
後攻プレイヤーの「初期手札枚数」, 「初期コスト」, 「最大 HP 」の変更がどれほど後攻プレイヤーの勝率に影響を及ぼすかを後攻の勝率が 55 \% を超えるまで調べた. 各パラメータの値の初期値はそれぞれ 3, 1, 20 である. なお, シミュレーションの際は 50000 回対戦を実行し, 10000 回おきに勝率を平均し 5 個の値の平均値を勝率とした.
表 \ref{table:paramresult1} に結果を示す.

\begin{table}[ht]
        \centering
        \caption{後攻プレイヤーのパラメータを変更することによる勝率の変化 ( 55 \% 超えは太字で示す)}
        \vspace{-0.3cm}
        \label{table:paramresult1}
        \scalebox{1.0}[1.0]{
          \begin{tabular}{|c|c|}
            \hline
            変更説明 & 勝率 \\ \hline
            初期値 & 0.49424 \\ \hline
            初期手札 3 → 4 & \textbf{0.59276} \\ \hline
            初期コスト 1 → 2 & \textbf{0.59096} \\ \hline
            HP 最大値 20 → 21 & 0.51692 \\ \hline
            HP 最大値 20 → 22 & 0.53216 \\ \hline
            HP 最大値 20 → 23 & 0.54676 \\ \hline
            HP 最大値 20 → 24 & \textbf{0.56362} \\ \hline
            HP 最大値 20 → 25 & \textbf{0.58058} \\ \hline
            HP 最大値 20 → 26 & \textbf{0.58834} \\ \hline
            HP 最大値 20 → 27 & \textbf{0.60598} \\ \hline
            \end{tabular}
        }
        \end{table}

初期手札枚数, 初期コストは値を 1 変えただけで後攻の勝率が 55 \% を超えて約 6 割まで増加した. HP の最大値を変更させた場合は 24 から勝率が 55 \% を超え始め, 26 , 27 付近で初期手札枚数, 初期コストを 1 変更した場合と同じ程度の勝率を記録した. \par
この変化の度合いをカードのパラメータ調整に反映したかったが, 各パラメータがどんな影響を及ぼしてどう調整に反映するのか検討がつかず, 現在行き詰まってます. (例えば初期手札枚数を変えて勝率が大きく変わったということは, 初期ドロー運に大きく左右されるということでデッキ全体のカードバランスが悪いと考えられるが, これをどうバランス調整に持ち込むのか)

\subsection{実験 2 }
実験 1 と趣向を変え, バランスブレイカーの検出およびデッキ全体のナーフバフによるバランス調整を検討した.\par
ハースストーンの異なるデッキ間のバランス調整に取り組んでいた研究\cite{Hearthstone} で用いられていた WRP (Win Rate when Played) を取り入れ, デッキ内のカードのバランスを確認した. 

表 \ref{table:deck} に示す実験で用いるデッキは以前から変わらない. 
\begin{table}[h]
  \centering
  \caption{デッキの内容}
  \label{table:deck}
  \begin{tabular}{|c|c|c|c|c|c|}
  \hline
  ID & 攻撃力 & HP & コスト & 特殊効果 & 枚数 \\ \hline
  0 & 1 & 1 & 0 & 無し & 2 \\ \hline
  1 & 2 & 1 & 1 & 無し & 2 \\ \hline
  2 & 3 & 2 & 2 & 無し & 2 \\ \hline
  3 & 4 & 3 & 3 & 無し & 2 \\ \hline
  4 & 5 & 4 & 4 & 無し & 2 \\ \hline
  5 & 2 & 2 & 2 & 召喚 & 2 \\ \hline
  6 & 2 & 3 & 3 & 召喚 & 2 \\ \hline
  7 & 1 & 1 & 1 & 循環 & 2 \\ \hline
  8 & 1 & 3 & 2 & 循環 & 2 \\ \hline
  9 & 2 & 1 & 2 & 速攻 & 2 \\ \hline
  10 & 3 & 1 & 3 & 速攻 & 2 \\ \hline
  11 & 1 & 2 & 2 & 攻撃 & 2 \\ \hline
  12 & 2 & 3 & 3 & 攻撃 & 2 \\ \hline
  13 & 1 & 1 & 1 & 治癒 & 2 \\ \hline
  14 & 2 & 1 & 3 & 治癒 & 2 \\ \hline
  \end{tabular}
  \end{table}

  カードの特殊効果は, 
  \begin{quote}
    \begin{itemize}
     \item 盤面に出したら (攻撃力 , HP) = ( 1 , 1 )のユニット追加で出す. (召喚)
     \item 盤面に出したら自プレイヤーの HP を 2 回復　(治癒)
     \item 盤面に出したら敵プレイヤーの HP を 2 削る　(攻撃)
     \item 盤面に出したら自プレイヤーは 1 枚カードをドロー　(循環)
     \item 盤面に出たターンに攻撃できる (速攻)
    \end{itemize}
   \end{quote}
   となっている.
  なお, 以下に結果として示しやすいようにカードに ID を割り振っている.
  デッキがミラーでランダムに対戦した場合の WRP を表 に示す. 

  \begin{table}[ht]
    \centering
    \caption{カードごとの WRP (降順)}
    \vspace{-0.3cm}
    \label{table:WRP}
    \scalebox{1.0}[1.0]{
      \begin{tabular}{|c|c|}
        \hline
        カード ID  & WRP \\ \hline
        4 & 0.56502 \\ \hline
        3 & 0.54911 \\ \hline
        6 & 0.54446 \\ \hline
        5 & 0.54069 \\ \hline
        12 & 0.53849 \\ \hline
        8 & 0.53489 \\ \hline
        2 & 0.53134 \\ \hline
        7 & 0.52547 \\ \hline
        11 & 0.51873 \\ \hline
        14 & 0.51707 \\ \hline
        10 & 0.51669 \\ \hline
        1 & 0.51444 \\ \hline
        13 & 0.51408 \\ \hline
        9 & 0.51043 \\ \hline
        0 & 0.50788 \\ \hline
        \end{tabular}
    }
    \end{table}
  この値の最大値と最小値が何らかの基準以下になるようにナーフとバフを繰り返すことで, バランス調整できそうと考えているが, カードにおける 攻撃力 , HP , コストのどの数値を調節すればよいのかが定まっていないためこの実験も行き詰まっている.\par
  2 つの実験に共通して, 調整するカードのパラメータ (攻撃力 , HP , コスト) を何にするか決定ができていない. 

\section{構築環境への DQN の適用}

研究発表会で頂いた指摘, 気づいた点など踏まえて修正して実験を行った. 
まず, アルゴリズム \ref{alg1} に示すように, 対戦相手の行動について相手のカードが全てプレイヤーに飛んできた時に負けの場合においては敵盤面の中で最も攻撃力が高いカードを攻撃するように改良した.
\begin{figure}[ht]
  \begin{algorithm}[H]
      \caption{敵の行動}
      \label{alg1}
      \begin{algorithmic}[1] 
      \FOR{手札のカード}
      \IF{盤面にプレイできる}
      \STATE カードをプレイ
      \ELSE
      \STATE pass
      \ENDIF
      \ENDFOR
      \FOR{自盤面のカード}
      \IF{敵の盤面に 1 回の攻撃で倒せるカードがある}
      \STATE そのカードを選んで攻撃
      \ELSE
      \IF{敵の盤面のカードの攻撃力の総和が自分の残り体力以上}
      \STATE 敵盤面の中で最も攻撃力が高いカードを攻撃
      \ELSE
      \STATE 敵プレイヤーを攻撃
      \ENDIF
      \ENDIF
      \ENDFOR
      \end{algorithmic}
  \end{algorithm}
\end{figure}
また, 実験結果においては勝率と学習時の報酬の推移だけでなく, エージェントのとった行動の総数, エージェントがプレイしたカードの種類とその総数を記録して行動の傾向を示すことができるようにした. 
アルゴリズム 1 の対戦相手に対して, 表 \ref{table:action2-2} に示す行動空間で 1000000 ステップ先攻で学習させ, 10000 回対戦を実行した.
\begin{table}[h]
  \centering
  \caption{実験で定義した行動空間}
  \label{table:action2-2}
  \begin{tabular}{|c|c|}
  \hline
  行動説明                          & 次元数        \\ \hline
  手札 1 $\sim$ 9 を自盤面に出す             & 9          \\ \hline
  自盤面 1 が敵盤面 1 $\sim$ 5 に攻撃 or 敵プレイヤーに攻撃    & 6          \\ \hline
  自盤面 2 が敵盤面 1 $\sim$ 5 に攻撃 or 敵プレイヤーに攻撃    & 6   \\ \hline
  自盤面 3 が敵盤面 1 $\sim$ 5 に攻撃 or 敵プレイヤーに攻撃    & 6\\ \hline
  自盤面 4 が敵盤面 1 $\sim$ 5 に攻撃 or 敵プレイヤーに攻撃    & 6 \\ \hline
  自盤面 5 が敵盤面 1 $\sim$ 5 に攻撃 or 敵プレイヤーに攻撃    & 6\\ \hline
  ターンエンド & 1 \\ \hline
  \end{tabular}
  \end{table}

結果は, 勝率が 0.9739 となった.
表 \ref{table:DQNCardCount} にエージェントがとった行動の総数の上位 5 個を示す. また, 表 \ref{table:DQNCardCount} に各カードにおけるエージェントがプレイした総数を示す. これらを追加することで考察に幅が出ると考えられる. 今回の実験では, 表 \ref{table:DQNCardCount} からわかるように相手プレイヤーをひたすら攻撃して先攻有利を押し付けている.

\begin{table}[ht]
  \centering
  \caption{先攻で 1000000 ステップ学習したエージェントの 10000 回の対戦実行における行動総数上位 5 つ (ターンエンドは除外)}
  \vspace{-0.3cm}
  \label{table:DQNFirstaction}
  \scalebox{1.0}[1.0]{
    \begin{tabular}{|c|c|}
      \hline
      行動説明 & 総数 \\ \hline
      手札 1 を盤面に出す & 76955 \\ \hline
      自盤面 1 で相手プレイヤーに攻撃 & 52531 \\ \hline
      自盤面 2 で相手プレイヤーに攻撃 & 27875 \\ \hline
      手札 2 を盤面に出す & 15191 \\ \hline
      自盤面 3 で相手プレイヤーに攻撃 & 10130 \\ \hline
      \end{tabular}
  }
  \end{table}

  \begin{table}[ht]
    \centering
    \caption{先攻で 1000000 ステップ学習したエージェントの 10000 回の対戦実行における各カードのプレイされた総数 (降順)}
    \vspace{-0.3cm}
    \label{table:DQNCardCount}
    \scalebox{1.0}[1.0]{
      \begin{tabular}{|c|c|}
        \hline
        カード ID  & 総数 \\ \hline
        0 & 8463 \\ \hline
        13 & 8069 \\ \hline
        1 & 7971 \\ \hline
        7 & 7845 \\ \hline
        11 & 7693 \\ \hline
        2 & 7672 \\ \hline
        5 & 7663 \\ \hline
        9 & 7483 \\ \hline
        8 & 7372 \\ \hline
        3 & 6910 \\ \hline
        12 & 6876 \\ \hline
        4 & 6862 \\ \hline
        6 & 6720 \\ \hline
        10 & 6630 \\ \hline
        14 &  6567 \\ \hline
        \end{tabular}
    }
    \end{table}


\section{今後の課題}
\begin{itemize}
  \item ゲームバランス調整
  \par 
  カードのどのパラメータを調整すればよいのか決定する方法を見つけることができていない.
  \item DQN 追加実験
  \par 
  アグロが最適解すぎるのでブロッキングと全破壊の特殊効果を追加した実験もしてみる.
\end{itemize}
%index.bibはtexファイルと同階層に置く
%ちゃんと\citeしないと表示されない(1敗)
\bibliography{index.bib}
\bibliographystyle{junsrt}

\end{document}