@article{Vehicle,
  author    = {Andreas Folkers and
               Matthias Rick and
               Christof B{\"{u}}skens},
  title     = {Controlling an Autonomous Vehicle with Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1909.12153},
  year      = {2019},
  url       = {http://arxiv.org/abs/1909.12153},
  eprinttype = {arXiv},
  eprint    = {1909.12153},
  timestamp = {Fri, 27 Sep 2019 13:04:21 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1909-12153.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@conference{robotics,
  title = {Deep Reinforcement Learning for Robotic Manipulation with Asynchronous Off-Policy Updates},
  author = {Gu*, Shixiang and Holly*, Ethan and Lillicrap, Timothy and Levine, Sergey},
  booktitle = {Proceedings of the IEEE International Conference on Robotics and Automation (ICRA)},
  publisher = {IEEE},
  address = {Piscataway, NJ, USA},
  month = may,
  year = {2017},
  note = {*equal contribution},
  doi = {10.1109/ICRA.2017.7989385},
  month_numeric = {5}
}
@inproceedings{recommendation,
author = {Zheng, Guanjie and Zhang, Fuzheng and Zheng, Zihan and Xiang, Yang and Yuan, Nicholas Jing and Xie, Xing and Li, Zhenhui},
title = {DRN: A Deep Reinforcement Learning Framework for News Recommendation},
year = {2018},
isbn = {9781450356398},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3178876.3185994},
doi = {10.1145/3178876.3185994},
abstract = {In this paper, we propose a novel Deep Reinforcement Learning framework for news recommendation. Online personalized news recommendation is a highly challenging problem due to the dynamic nature of news features and user preferences. Although some online recommendation models have been proposed to address the dynamic nature of news recommendation, these methods have three major issues. First, they only try to model current reward (e.g., Click Through Rate). Second, very few studies consider to use user feedback other than click / no click labels (e.g., how frequent user returns) to help improve recommendation. Third, these methods tend to keep recommending similar news to users, which may cause users to get bored. Therefore, to address the aforementioned challenges, we propose a Deep Q-Learning based recommendation framework, which can model future reward explicitly. We further consider user return pattern as a supplement to click / no click label in order to capture more user feedback information. In addition, an effective exploration strategy is incorporated to find new attractive news for users. Extensive experiments are conducted on the offline dataset and online production environment of a commercial news recommendation application and have shown the superior performance of our methods.},
booktitle = {Proceedings of the 2018 World Wide Web Conference},
pages = {167–176},
numpages = {10},
keywords = {news recommendation, deep Q-Learning, reinforcement learning},
location = {Lyon, France},
series = {WWW '18}
}
@article{AlphaGo,
  added-at = {2017-12-15T02:14:58.000+0100},
  author = {Silver, David and Schrittwieser, Julian and Simonyan, Karen and Antonoglou, Ioannis and Huang, Aja and Guez, Arthur and Hubert, Thomas and Baker, Lucas and Lai, Matthew and Bolton, Adrian and Chen, Yutian and Lillicrap, Timothy and Hui, Fan and Sifre, Laurent and van den Driessche, George and Graepel, Thore and Hassabis, Demis},
  biburl = {https://www.bibsonomy.org/bibtex/2ecdfbfcceb55ee5f14c1c375ad71f2cb/achakraborty},
  description = {Mastering the game of Go without human knowledge | Nature},
  interhash = {c45d318e105d0f2d62ccc28c2699d9d4},
  intrahash = {ecdfbfcceb55ee5f14c1c375ad71f2cb},
  journal = {Nature},
  keywords = {2017 deep-learning deepmind google paper reinforcement-learning},
  month = oct,
  pages = {354--},
  publisher = {Macmillan Publishers Limited, part of Springer Nature. All rights reserved.},
  timestamp = {2017-12-15T02:14:58.000+0100},
  title = {Mastering the game of Go without human knowledge},
  url = {http://dx.doi.org/10.1038/nature24270},
  volume = 550,
  year = 2017
}
@article{AlphaZero,
  added-at = {2018-12-13T08:23:43.000+0100},
  author = {Silver, David and Hubert, Thomas and Schrittwieser, Julian and Antonoglou, Ioannis and Lai, Matthew and Guez, Arthur and Lanctot, Marc and Sifre, Laurent and Kumaran, Dharshan and Graepel, Thore and others},
  biburl = {https://www.bibsonomy.org/bibtex/2cada5fafbda156b2022a6b561174455e/loroch},
  interhash = {9cc5dc9a85ded11f41db1659d3ba0c23},
  intrahash = {cada5fafbda156b2022a6b561174455e},
  journal = {Science},
  keywords = {AlphaZero RL chess deep_learning deepmind go shogi},
  number = 6419,
  pages = {1140--1144},
  publisher = {American Association for the Advancement of Science},
  timestamp = {2018-12-13T08:23:43.000+0100},
  title = {A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play},
  url = {http://science.sciencemag.org/content/362/6419/1140/tab-pdf},
  volume = 362,
  year = 2018
}
@ARTICLE{OpenAIGym,
       author = {{Brockman}, Greg and {Cheung}, Vicki and {Pettersson}, Ludwig and {Schneider}, Jonas and {Schulman}, John and {Tang}, Jie and {Zaremba}, Wojciech},
        title = "{OpenAI Gym}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
         year = 2016,
        month = jun,
          eid = {arXiv:1606.01540},
        pages = {arXiv:1606.01540},
archivePrefix = {arXiv},
       eprint = {1606.01540},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv160601540B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{DQN,
  abstract = {We present the first deep learning model to successfully learn control
policies directly from high-dimensional sensory input using reinforcement
learning. The model is a convolutional neural network, trained with a variant
of Q-learning, whose input is raw pixels and whose output is a value function
estimating future rewards. We apply our method to seven Atari 2600 games from
the Arcade Learning Environment, with no adjustment of the architecture or
learning algorithm. We find that it outperforms all previous approaches on six
of the games and surpasses a human expert on three of them.},
  added-at = {2019-07-12T20:11:01.000+0200},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  biburl = {https://www.bibsonomy.org/bibtex/2a00ec4c09f5dc9b3f8a1836f4e02bb5d/lanteunis},
  interhash = {78966703f649bae69a08a6a23a4e8879},
  intrahash = {a00ec4c09f5dc9b3f8a1836f4e02bb5d},
  keywords = {},
  note = {cite arxiv:1312.5602Comment: NIPS Deep Learning Workshop 2013},
  timestamp = {2019-07-12T20:11:01.000+0200},
  title = {Playing Atari with Deep Reinforcement Learning},
  url = {http://arxiv.org/abs/1312.5602},
  year = 2013
}
@article{DQN_algorithm,
  added-at = {2015-08-26T14:46:40.000+0200},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A. and Veness, Joel and Bellemare, Marc G. and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K. and Ostrovski, Georg and Petersen, Stig and Beattie, Charles and Sadik, Amir and Antonoglou, Ioannis and King, Helen and Kumaran, Dharshan and Wierstra, Daan and Legg, Shane and Hassabis, Demis},
  biburl = {https://www.bibsonomy.org/bibtex/2fb15f4471c81dc2b9edf2304cb2f7083/hotho},
  description = {Human-level control through deep reinforcement learning - nature14236.pdf},
  interhash = {eac59980357d99db87b341b61ef6645f},
  intrahash = {fb15f4471c81dc2b9edf2304cb2f7083},
  issn = {00280836},
  journal = {Nature},
  keywords = {deep learning toread},
  month = feb,
  number = 7540,
  pages = {529--533},
  publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  timestamp = {2015-08-26T14:46:40.000+0200},
  title = {Human-level control through deep reinforcement learning},
  url = {http://dx.doi.org/10.1038/nature14236},
  volume = 518,
  year = 2015
}
@ARTICLE{NSGA-2,
  author={Deb, K. and Pratap, A. and Agarwal, S. and Meyarivan, T.},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={A fast and elitist multiobjective genetic algorithm: NSGA-II}, 
  year={2002},
  volume={6},
  number={2},
  pages={182-197},
  doi={10.1109/4235.996017}
}
@article{osero,
   author	 = "甲野,佑 and 田中,一樹 and 岡田,健 and 奥村,エルネスト 純",
   title	 = "“逆転オセロニア”における深層強化学習応用",
   journal	 = "デジタルプラクティス",
   year 	 = "2019",
   volume	 = "10",
   number	 = "2",
   pages	 = "351--367",
   month	 = "jan"
}
@article{EvolvingHearthStone,
  author    = {Fernando de Mesentier Silva and
               Rodrigo Canaan and
               Scott Lee and
               Matthew C. Fontaine and
               Julian Togelius and
               Amy K. Hoover},
  title     = {Evolving the Hearthstone Meta},
  journal   = {CoRR},
  volume    = {abs/1907.01623},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.01623},
  eprinttype = {arXiv},
  eprint    = {1907.01623},
  timestamp = {Mon, 08 Jul 2019 14:12:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-01623.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@article{NSGA2,
author="阪井, 節子 and 高濱, 徹行",
title="多目的最適化手法NSGA-2における等距離選択の効果について",
journal="数理解析研究所講究録",
ISSN="18802818",
publisher="京都大学数理解析研究所",
year="2008",
month="04",
number="1589",
pages="53-64",
URL="https://cir.nii.ac.jp/crid/1520290885005742080"
}
@article{MuZero,
  author    = {Julian Schrittwieser and
               Ioannis Antonoglou and
               Thomas Hubert and
               Karen Simonyan and
               Laurent Sifre and
               Simon Schmitt and
               Arthur Guez and
               Edward Lockhart and
               Demis Hassabis and
               Thore Graepel and
               Timothy P. Lillicrap and
               David Silver},
  title     = {Mastering Atari, Go, Chess and Shogi by Planning with a Learned Model},
  journal   = {CoRR},
  volume    = {abs/1911.08265},
  year      = {2019},
  url       = {http://arxiv.org/abs/1911.08265},
  eprinttype = {arXiv},
  eprint    = {1911.08265},
  timestamp = {Mon, 02 Dec 2019 17:48:37 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1911-08265.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@ARTICLE{ReBeL,
       author = {{Brown}, Noam and {Bakhtin}, Anton and {Lerer}, Adam and {Gong}, Qucheng},
        title = "{Combining Deep Reinforcement Learning and Search for Imperfect-Information Games}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
         year = 2020,
        month = jul,
          eid = {arXiv:2007.13544},
        pages = {arXiv:2007.13544},
archivePrefix = {arXiv},
       eprint = {2007.13544},
 primaryClass = {cs.GT},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2020arXiv200713544B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@inproceedings{Rainbow,
author = {Hessel, Matteo and Modayil, Joseph and van Hasselt, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
title = {Rainbow: Combining Improvements in Deep Reinforcement Learning},
year = {2018},
isbn = {978-1-57735-800-8},
publisher = {AAAI Press},
abstract = {The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.},
booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
articleno = {393},
numpages = {8},
location = {New Orleans, Louisiana, USA},
series = {AAAI'18/IAAI'18/EAAI'18}
}
@misc{DreamerV2,
  doi = {10.48550/ARXIV.2010.02193},
  
  url = {https://arxiv.org/abs/2010.02193},
  
  author = {Hafner, Danijar and Lillicrap, Timothy and Norouzi, Mohammad and Ba, Jimmy},
  
  keywords = {Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Mastering Atari with Discrete World Models},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@article{TDGA,
  title={熱力学的遺伝アルゴリズムによる多目的最適化},
  author={森 直樹 and 薮本 靖之 and 喜多 一 and 西川 緯一},
  journal={システム制御情報学会論文誌},
  volume={11},
  number={3},
  pages={103-111},
  year={1998},
  doi={10.5687/iscie.11.103}
}
@article{Optuna,
  author    = {Takuya Akiba and
               Shotaro Sano and
               Toshihiko Yanase and
               Takeru Ohta and
               Masanori Koyama},
  title     = {Optuna: {A} Next-generation Hyperparameter Optimization Framework},
  journal   = {CoRR},
  volume    = {abs/1907.10902},
  year      = {2019},
  url       = {http://arxiv.org/abs/1907.10902},
  eprinttype = {arXiv},
  eprint    = {1907.10902},
  timestamp = {Thu, 01 Aug 2019 08:59:33 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1907-10902.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@MastersThesis{GA説明,
  author={{足立 拓真}},
  title={分散ネットワークゲーム環境における戦略の進化},
  school={大阪府立大学大学院工学研究科電気・情報系専攻知能情報工学分野},
  year={2001},
}