\newpage
\changeindent{0cm}
\section{まとめと今後の課題}

本研究では, 3 つの手法を提案し, 数値実験用の独自の TCG 環境下で数値実験し提案手法の有効性を検証した.
\par
実験 1 では DQN を用いて人力で構築した戦略よりも高い勝率を記録するエージェントを構築した. また, DQN により構築したエージェントはデッキにおいて人間から見ても妥当な戦略を構築しており, 構築戦略下におけるカードの強弱も学習していることが分かった.
\par
実験 2 では提案手法 2 の数値実験をした. 人力で構築した戦略と DQN で構築した戦略では結果に差があった. DQN ではカードの強弱を学習しているためより人間のプレイに近い結果を得られるといえる.
\par
実験 3 では実験 2 で得られた結果を利用して提案手法 3 の数値実験をした. 調整するカード枚数を増やして GA の解空間の次元を増やすほど勝率に関する適応度は高くなっていた. 
また, 提案手法により得られた解は $f_w$ に関しては多目的 GA により得られた解に優越し, $f_c$ については最も優越する解が得られ, 提案手法の有効性が確かめられた.\par
今後の課題を以下に列挙する.
\begin{itemize}
  \item DQN 以外の深層強化学習手法の適用
  \par
  本研究ではエージェント構築の部分に DQN を適用した. 近年では, DQN に様々な工夫を加えた Rainbow \cite{Rainbow}, ターン制完全情報ゲームで顕著な成果を残している AlphaZero \cite{AlphaZero}, その他にも DreamerV2  \cite{DreamerV2}, MuZero \cite{MuZero}, RebeL \cite{ReBeL} など様々な優れた深層強化手法が提案されている. これらの手法を用いることで DQN と比べてより良い戦略を持つエージェントを構築することが期待できる.
  \item GA 以外の最適化手法の適用\par
  本研究では, ゲームバランスの調整においてデッキ内のカードのパラメータを対象として GA を適用した. GA には初期収束という問題がある. GA の選択ルールに熱力学的
  なエントロピーと温度の概念を取り入れることでこれを解決する TDGA \cite{TDGA} を用いることで提案手法 3 においてより良い解が得られると期待できる.
  \newpage
  \item ハイパーパラメータの最適化
  \par
  本研究では, DQN や GA といったアルゴリズムのハイパーパラメータを人力で設定した. Optuna \cite{Optuna} といったハイパーパラメータのチューニングツールを用いて本研究のタスクに適したハイパーパラメータを見つけることでより良い結果を得ることが期待できる.
  \item デッキ間の相性を考慮したゲームバランス調整方法の検討
  \par
  現在はデッキ間の勝率がどの対戦においても勝率が 50 \% に近いほうがゲームバランスとして良い状態としているが, 実際はデッキ間の相性があり, それを事前に推測するメタゲームという概念がある. このような相性まで考慮したバランス調整が重要である.
\end{itemize}




\changeindent{2cm}