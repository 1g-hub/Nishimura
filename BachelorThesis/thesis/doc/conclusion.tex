\newpage
\changeindent{0cm}
\section{まとめと今後の課題}

本研究では, 3 つの手法を提案し, 数値実験用の独自の TCG 環境下で数値実験し提案手法の有効性を検証した.
\par
実験 1 では DQN を用いてエージェントを構築し, 人力で構築した戦略よりも高い勝率を記録するエージェントを構築した. また, DQN によりエージェントがデッキにおいて人間から見ても妥当な戦略を構築しており, 構築戦略下におけるカードの強弱も学習していることが分かった.
\par
実験 2 では提案手法 2 の数値実験をした. 人力で構築した戦略と DQN で構築した戦略では結果に差があった. DQN ではカードの強弱を学習しているためより人間のプレイに近い結果を得られると考えられる.
\par
実験 3 では実験 2 で得られた結果を利用して提案手法 3 の数値実験をした. 調整するカード枚数を増やして GA の解空間の次元を増やすほど勝率に関する適応度は高くなっていた. 
また, 関連研究で用いられていたデッキのカード全てを対象とした単目的 GA, 多目的 GA に関して勝率に関する適応度に関しては中間的, 調整されたカード枚数については最も優越する解が得られ, 提案手法の有効性が確かめられた.\par
今後の課題を以下に列挙する.
\begin{itemize}
  \small
  \setlength{\itemsep}{0cm} % 項目間
  \item DQN 以外の深層強化学習手法の適用
  \par
  本研究ではエージェント構築の部分に DQN を適用した. 近年では, DQN に様々な工夫を加えた Rainbow \cite{Rainbow}, ターン制完全情報ゲームで顕著な成果を残している AlphaZero , その他にも DreamerV2, MuZero, RebeL など様々な優れた深層強化手法が提案されている \cite{DreamerV2}\cite{MuZero}\cite{ReBeL}. これらの手法を用いることで DQN と比べてより良い戦略を持つエージェントを構築することが期待できる.
  \item GA 以外の最適化手法の適用\par
  GA には初期収束という問題がある. GA の選択ルールに熱力学的
  なエントロピーと温度の概念を取り入れることでこれを解決する TDGA \cite{TDGA} を用いることで提案手法 3 においてより良い解が得られると期待できる.
  \item ハイパーパラメータの最適化
  \par
  本研究では, DQN や GA といったアルゴリズムのハイパーパラメータを人力で設定した. Optuna \cite{Optuna} といったハイパーパラメータのチューニングツールを用いて本研究のタスクに適したハイパーパラメータを見つけることでより良い結果を得ることが期待できる.
\end{itemize}




\changeindent{2cm}