%\documentstyle[epsf,twocolumn]{jarticle}       %LaTeX2.09仕様
%\documentclass[twocolumn]{jarticle}     %pLaTeX2e仕様
\documentclass[twocolumn]{jarticle}     %pLaTeX2e仕様

%一枚組だったら[twocolumn]関係のとこ消す

\setlength{\topmargin}{-45pt}
%\setlength{\oddsidemargin}{0cm} 
\setlength{\oddsidemargin}{-7.5mm}
%\setlength{\evensidemargin}{0cm} 
\setlength{\textheight}{24.1cm}
%setlength{\textheight}{25cm} 
\setlength{\textwidth}{17.4cm}
%\setlength{\textwidth}{172mm} 
\setlength{\columnsep}{11mm}

\kanjiskip=.07zw plus.5pt minus.5pt

\usepackage[dvipdfm]{graphicx}
\usepackage{ccaption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{subcaption}
\usepackage{enumerate}
\usepackage{comment}
\usepackage{url}
\usepackage{multirow}
\usepackage{diagbox}
\usepackage{amssymb}
\usepackage{mathtools}
\usepackage{wrapfig}
\usepackage{graphicx}
\usepackage{float}
\usepackage{amsmath}
\usepackage{lipsum}


\begin{document}
  \noindent
  \twocolumn[
    \hspace{1em}

    前期研究会発表資料 令和 5 年 5 月 31 日 (水)
    \hfill
    \ \  M1 西村昭賢 

    \vspace{2mm}
    \hrule
    \begin{center}
    {\Large \bf 深層強化学習に基づくトレーディングカードゲーム環境の構築}
    \end{center}
    \hrule
    \vspace{3mm}
  
  ]

  \section{はじめに}
  近年, ゲーム環境への深層強化学習の応用が注目されている．特に，プレイヤーが得られる情報が部分的である不完全情報ゲームへと積極的に応用されている.
  本研究では不完全情報ゲームの 1 つであるトレーディングカードゲーム (TCG) に着目した. TCG は使用可能なカードの性能や種類を変更可能という点で, 他のゲームよりも人工知能による攻略が困難である. また, この性質のためゲームバランスの調整が難しく, 公開後に修正が入ることが一般的であり, カードの性能を上方修正するバフや下方修正するナーフなどの用語が用いられる. \par
  上記の背景から, 本研究では深層強化学習とそれに基づく進化型計算を用いた TCG 環境のゲームバランス最適化手法を提案し, 独自の TCG 環境を用いた数値実験により提案手法の有効性を検証した. 

  \section{要素技術}
\subsection{Deep Q Network}
Deep Q Network (DQN) は価値ベースの代表的な強化学習手法の Q 学習と深層学習を融合させた代表的な深層強化学習手法である \cite{DQN}.
\subsection{Genetic Algorithm}
Genetic Alogorithm (GA) とは, 生物の進化とその過程を模した最適化手法であり, 主に組合せ最適化問題に対して適用される.  NSGA-II \cite{NSGA-2} といった多目的最適化 GA も提案されている.

\section{提案手法}
\subsection{提案手法 1 : 数値実験用の TCG 環境}
現在ではTCGをデジタル化したデジタルカードゲーム (DCG) もあるが, 本研究ではすべて TCG として扱う.
Magic : The Gathering \footnote[1]{https://magic.wizards.com} に代表される一般的な TCG と同様に, ゲームは 2 人のプレイヤーからなり, プレイヤーは複数のカードからなるデッキを持つ. 
プレイヤーは手札, 盤面と呼ばれるカードを保有する領域を持ち, ドローと呼ばれる操作でカードをデッキから手札に加える. また, プレイと呼ばれる操作でカードを手札から盤面に出す. また, デッキからカードが無くなった状態をデッキ切れと呼ぶ.  
また, プレイヤー自身が HP, マナという 2 つの整数値パラメータを持つ.
今回の環境ではプレイヤーの HP の最大値は 20, マナの上限値は初期値が 1 で最大値を 5 と設定した.
\par
また TCG では, カードは場に残るユニットと使い切りのスペルの 2 種類のカードに分けられる. 本研究ではユニットのみを考慮した.
カードはそれぞれ攻撃力と HP とコストの 3 つの整数値パラメータを持つ. カードがプレイされる際, プレイヤーはカードのコスト分マナを減少させる. またマナが負の値になる場合はプレイすることができない.
また, 盤面にあるカードは対戦相手の盤面にあるカード, あるいは相手プレイヤーに攻撃することができる. カードが攻撃する際には, 攻撃対象の HP へとカードの持つ攻撃力分ダメージを与える. またカードへと攻撃する際には攻撃対象のカードが持つ攻撃力分, 攻撃するカードもダメージを受ける.
カードが攻撃が可能となるのはプレイされたターンの次のターンからとなる. 
カードの HP が 0 になった, あるいは後述する手札と盤面の枚数制限を超えて手札にドロー, 盤面にプレイされた時はカードは破壊される. 破壊されたカードはゲームから取り除かれる. 
また, カードによっては以下に示す特殊効果を持つものもある.

\begin{description}
   \vspace{-0.15cm}
   \setlength{\itemsep}{0cm}
   \small
  \item[召喚 :]  プレイ時 (攻撃力 , HP) = (1 , 1) の
  ユニットを追加でプレイ
  \item[治癒 :]  プレイ時自プレイヤーの HP を 2 回復する
  \item[攻撃 :]  プレイ時敵プレイヤーの HP を 2 減らす
  \item[取得 :]  プレイ時自プレイヤーは 1 枚カードをドローする
  \item[速攻 :]  プレイされたターンに攻撃できる
\end{description}
\vspace{-0.3cm}
また, ゲームの流れは以下のようになっている.
\begin{enumerate}
  \small
  \vspace{-0.15cm}
  \setlength{\itemsep}{0cm} % 項目間
  \item ゲーム開始時に各プレイヤーは自身のデッキをシャッフル.
  \item デッキから初期手札としてカードを 5 枚ドロー. 
  \item 先攻プレイヤーは 1 ターン目のドローステップをスキップし行動.
  \item 後攻プレイヤーはカードを 1 枚ドローして行動.
  \item 2 ターン目以降は先攻プレイヤーもカードを 1 枚ドローしてから行動. 
  \item 4 , 5 の繰り返し. なお, ターンプレイヤーは行動前にマナを上限値まで回復. このときマナの上限値が 5 でなければ上限値を 1 増やしてから回復.
  \item プレイヤーがデッキ切れになっている状態でカードをドローしようとした, あるいはプレイヤー自身の HP が 0 となった場合はそのプレイヤーが敗北となりゲーム終了.
\end{enumerate}
\vspace{-0.15cm}
\par
本構築環境では一般的な TCG と同様にカードがプレイされた次のターンから行動可能となるため, 先攻プレイヤーがカードの行動が早くなり有利となる.そのため, 先攻の 1 ターン目のドローステップをスキップしている. 

\subsection{提案手法 2 : DQN によるデッキ内のカードパワーの定量的な評価方法}
\label{manaratio}
TCG においてデッキ内の各カードのカードパワーを測る一般的な指標としてカードの HP を $h$, 攻撃力を $a$, コストを $c$ とすると
\begin{equation*}
   \small
   r_\mathrm{mana} = \frac{h+a}{2c}
\end{equation*}
として数値化されるマナレシオ $r_\mathrm{mana}$ がある. 
しかし, マナレシオはカードの特殊効果といった要素を考慮していないためあくまで目安にしかならない. \par
本研究では, TCG 環境において DQN を用いた定量的なカードパワー評価指標を提案する.
具体的には, DQN を用いてカードパワーを測定したいデッキにおける妥当な戦略を持つエージェントを構築する. 
そしてそのエージェント同士を先攻後攻両方に配置し, 先攻後攻ごとにそれぞれ 1 種類ずつカードを除いて勝率を計算する. これによりデッキ内のカードの種類数 $n_{\mathrm{card}}$ について, $n_{\mathrm{card}}^2$ 個の勝率の値を得ることができ, これらの値から定量的に構築戦略下のカードパワーを評価する. 

\subsection{提案手法3 : 調整するカード枚数を最小限に抑えた TCG 環境のゲームバランス最適化手法}
\label{hearthstone}
前提として, 本研究では TCG 環境においてデッキ間の勝率が $50\%$ に近いことが TCG 環境におけるゲームバランスとして好ましい状態とした. なお実際にはデッキ間の相性が存在し必ずしもこのような状態にならないことに留意すべきである.
\par
Fernando らは HearthStone \footnote[2]{https://hearthstone.blizzard.com} の TCG 環境内においてデッキ間の勝率が 50 \% となるように, GA を用いてすべてのデッキ内のすべてのカードのパラメータを調整した \cite{Hearthstone}.
結果として, バランス調整の際の変更が多いとユーザーに対する影響が多いため好ましくないという考えのもと, 多目的 GA を用いることでパラメータの変更量を減らしながらデッキ間の勝率を最適化していた.
しかし, 関連研究の手法ではデッキ内のすべてのカードを対象とし, どのカードにも調整が起こりえる.
\par
そこで本研究では, TCG 環境のゲームバランス最適化の過程において変更量ではなく調整されるカードの枚数を抑えた方が, ユーザーにとって好ましいと考え, 調整されるカードの枚数を最小限にするような TCG 環境のゲームバランス最適化手法を提案する.
具体的には, 提案手法 2 によって得られたカードパワーの評価から調整されるカードを限定して GA の解空間を削減することで, 直接的に調整されるカードの数を減らす.
\vspace{-0.20cm}

\section{実験方法}
\subsection{実験 1}
学習側のデッキに \ref{manaratio} 節で述べたマナコストの観点において強いカードを 1 種類, 弱いカードを恣意的に 1 種類ずつ入れ, 提案する TCG 環境において DQN を適用した. 後攻プレイヤーの行動のみを学習し, 学習後 10000 回対戦を実行した. 学習中, 学習後の対戦において先攻プレイヤーには筆者らが実装したアグロ, コントロールと呼ばれる戦略を持つエージェントを配置している. 本稿では, アグロは相手プレイヤーへの攻撃を優先する戦略で, コントロールは相手盤面のカードの処理を優先する戦略である. また, 先攻プレイヤーは 1 エピソードごとに等確率で戦略を変化させ, 戦略に応じて事前に戦略間の勝率が $50 \pm 5\%$ となるよう調整されたデッキを持つ. 
実験から学習済エージェントの勝率を記録し, ベースラインと比較し DQN の妥当性を確かめた. また対戦した記録から選択した行動, 各カードのプレイされた回数を計測し学習序盤のエージェントと比較することで学習済エージェントの行動を分析した.
\begin{table}[t]
   \centering
   \caption{学習側プレイヤーのデッキ}
   \label{table:OPdeck}
   \vspace{-0.3cm}
   \scalebox{0.70}[0.70]{
     \begin{tabular}{|c|c|c|c|c|c|}
       \hline
       ID & 攻撃力 & HP & コスト & 特殊効果 & 枚数 \\ \hline
       0 & 4 & 4 & 1 & 無し & 2 \\ \hline
       1 & 2 & 2 & 2 & 無し & 2 \\ \hline
       2 & 3 & 3 & 3 & 無し & 2 \\ \hline
       3 & 4 & 3 & 4 & 無し & 2 \\ \hline
       4 & 5 & 4 & 5 & 無し & 2 \\ \hline
       5 & 2 & 2 & 2 & 召喚 & 2 \\ \hline
       6 & 2 & 3 & 3 & 召喚 & 2 \\ \hline
       7 & 1 & 1 & 1 & 取得 & 2 \\ \hline
       8 & 1 & 3 & 2 & 取得 & 2 \\ \hline
       9 & 2 & 1 & 2 & 速攻 & 2 \\ \hline
       10 & 3 & 1 & 3 & 速攻 & 2 \\ \hline
       11 & 1 & 2 & 2 & 攻撃 & 2 \\ \hline
       12 & 2 & 3 & 3 & 攻撃 & 2 \\ \hline
       13 & 1 & 1 & 1 & 治癒 & 2 \\ \hline
       14 & 1 & 1 & 5 & 治癒 & 2 \\ \hline
       \end{tabular}
   }
   
   \end{table}
また, DQN におけるエージェントの状態空間, 行動空間は, 手札と盤面にそれぞれ 5, 9 枚の枚数上限を設けて定義した. 表 \ref{table:state}, \ref{table:action} に本研究における状態空間と行動空間の定義を示す. \par
また, 報酬 $r$ はゲーム開始から勝敗が決するまでを 1 エピソードとして, 1 ステップ終了後は $r = 0.0$, 1 エピソード終了後にエージェントが勝っていれば $r = 1.0$, 負けていれば $r = -1.0$ と設定した.

\begin{table}[t]
   \small
   \centering
   \caption{定義した状態空間}
   \label{table:state}
   \vspace{-0.3cm}
   \scalebox{0.80}[0.80]{
     \begin{tabular}{|c|c|c|c|}
       \hline
       状態説明                        & 次元数        & 最小値        & 最大値         \\ \hline \hline
       各プレイヤーの HP & 2 & 0 & 20 \\ \hline
       各プレイヤーの マナ & 2 & 0 & 5 \\ \hline
       \begin{tabular}{c}
         手札 1 $\sim$ 9 の\\HP , 攻撃力, コスト, 特殊効果
         \end{tabular}      & 36         & 0          & 5          \\
       \hline
       \begin{tabular}{c}
         自盤面 1 $\sim$ 5 の\\HP と攻撃力
       \end{tabular}     & 10         & 0          & 5 \\
       \hline
       \begin{tabular}{c}
         敵盤面 1 $\sim$ 5 の\\HP と攻撃力
       \end{tabular}     & 10         & 0          & 5 \\
       \hline
       \begin{tabular}{c}
         自盤面 1 $\sim$ 5 が\\攻撃可能かどうか
       \end{tabular} & 5          & 0          & 1  \\
       \hline
       \begin{tabular}{c}
         お互いのデッキの\\残り枚数
       \end{tabular}     & 2 & 0 & 30 \\
        \hline
       \end{tabular}
   }
   \end{table}
 
   \begin{table}[t]
     \centering
     \caption{定義した行動空間}
     \vspace{-0.3cm}
     \label{table:action}
     \scalebox{0.80}[0.80]{
       \begin{tabular}{|c|c|}
         \hline
         行動説明                          & 次元数        \\ \hline \hline
         手札 1 $\sim$ 9 を自盤面にプレイ             & 9          \\ \hline
         自盤面 1 で敵盤面 1 $\sim$ 5 に攻撃or敵プレイヤーに攻撃    & 6          \\ \hline
         自盤面 2 で敵盤面 1 $\sim$ 5 に攻撃or敵プレイヤーに攻撃    & 6          \\ \hline
         自盤面 3 で敵盤面 1 $\sim$ 5 に攻撃or敵プレイヤーに攻撃    & 6          \\ \hline
         自盤面 4 で敵盤面 1 $\sim$ 5 に攻撃or敵プレイヤーに攻撃    & 6          \\ \hline
         自盤面 5 で敵盤面 1 $\sim$ 5 に攻撃or敵プレイヤーに攻撃    & 6          \\ \hline
         ターンエンド & 1 \\ \hline
         \end{tabular}
     }
       \end{table}
       また, 表 \ref{table:dqnparam} に実験 1 で用いた DQN のパラメータを示す.
       今回の実験において, $\mathrm{\epsilon\textrm{-}greedy}$ における $\epsilon$ は (\ref{epsilon}) 式に従って, $\mathrm{\epsilon_{max}}$ から $\mathrm{\epsilon_{min}}$ へと指数関数的に減少するように設定した. 
       \begin{equation}
         \vspace{-0.3cm}
         \label{epsilon}
         \epsilon = \mathrm{max}(\mathrm{\epsilon_{min}} \: , \: \mathrm{\epsilon_{min}} + (\mathrm{\epsilon_{max} - \epsilon_{min}}) \exp(- \frac{n_{\mathrm{step}}}{\mathrm{\epsilon_{decay}}}))
       \end{equation}
       ここで ${n_\mathrm{step}}$ は学習時の累積ステップ数を表す. 本実験では $\mathrm{\epsilon_{max}} = 1.0, \mathrm{\epsilon_{min}} = 0.1, \mathrm{\epsilon_{decay}} = 50000$ とした.
       \begin{table}[t]
         \centering
         \caption{DQNのパラメータ}
         \vspace{-0.3cm}
         \label{table:dqnparam}
         \scalebox{0.70}[0.70]{
           \begin{tabular}{|c|c|}
             \hline
             パラメータ名 & 値 \\ \hline \hline
             割引率 $\gamma$ & 0.99 \\ \hline     
             全結合層の活性化関数             & ReLU     \\ \hline
             全結合層の次元                & 64       \\ \hline
             最適化アルゴリズム              & Adam     \\ \hline
             方策                 & $\epsilon$-greedy \\ \hline
             Target Network 更新重み              & 0.5     \\ \hline
             Exprience Memory 開始ステップ数 & $1.0 \times 10^5$ \\ \hline
             学習ステップ数 &  $1.0 \times 10^6$ \\ \hline
             \end{tabular}
         }
         \end{table}
\subsection{実験 2}
実験 2 では実験 1 で構築した学習済エージェントをそのまま利用する. 
エージェントを先攻後攻両方に配置し, 表 \ref{table:OPdeck} のデッキを両方に持たせる. それぞれデッキからカードを 1 種類ずつ除いて 10000 回対戦を実行し先攻側の勝率を記録する. また, 比較対象として構築したエージェントだけでなく筆者が作成したアグロの戦略を持つプレイヤーにおいても表 \ref{table:OPdeck} のデッキを持たせて先攻後攻両方に配置し, 同様にカードを 1 種類ずつ除いた際の勝率を記録する.

\subsection{実験 3}
TCG 環境に表 \ref{table:OPdeck} のデッキをアグロ用のデッキとして追加し, デッキ間の勝率を 50 \% に近づけるようにゲームバランスを調整するという問題を設定する. この問題設定下でカードの調整枚数を限定してデッキ間の勝率を最適化する単目的 GA を適用した. また, 比較手法として \ref{hearthstone} 節で述べた関連研究で用いられていた表 \ref{table:OPdeck} の 15 種類のカードすべてを対象とした, 勝率を最適化する単目的 GA および勝率とパラメータの変更量の 2 つを最適化する多目的 GA を適用した.
ゲームバランスの調整において GA で調整するパラメータは表 \ref{table:OPdeck} の 15 種類のカードの HP, 攻撃力, コストとした.
\begin{table}[t]
   \centering
   \caption{実験 3 の問題設定における TCG 環境}
   \label{jikken3env}
   \vspace{-0.3cm}
   \scalebox{0.75}[0.75]{
     \begin{tabular}{|c|c|c|c|}
       \hline
       \diagbox[]{先攻}{後攻} &  新追加デッキ (アグロ)    & アグロ    & コントロール \\ \hline
       新追加デッキ(アグロ) & $r_{0}$ & $r_{1}$ & $r_{2}$ \\ \hline
       アグロ &   $r_{3}$  & 0.5255 & 0.5424 \\ \hline
       コントロール& $r_{4}$ & 0.5121 & 0.5053 \\ \hline
       \end{tabular}
   }
   \end{table} 
   \begin{table}[t]
     \centering
     \caption{GAのパラメータ}
     \vspace{-0.3cm}
     \label{table:gaparam}
     \scalebox{0.65}[0.70]{
       \begin{tabular}{|c|c|}
         \hline
         パラメータ名 & 値 \\ \hline \hline
         世代数 & 50 \\ \hline     
         個体数 & 50     \\ \hline
         遺伝子長 & 調整するカードの種類数 $\times$ 3       \\ \hline
         交叉率 & 0.4 \\ \hline
         交叉の種類 & 2 点交叉 \\ \hline
         個体ごとの突然変異率 & 0.2 \\ \hline
         選択 & 1 個体だけエリート保存, その他はトーナメント方式 \\ \hline
         トーナメントサイズ &  3 \\ \hline
         多目的 GA のアルゴリズム & NSGA-II \\ \hline
         \end{tabular}
     }
\end{table}
   
また, 単目的 GA における適応度, 多目的 GA における目的関数として各解においてデッキ間の勝率に関する適応度 $f_\mathrm{w}$,  パラメータの変更量に関する適応度 $f_\mathrm{p}$, 調整されたカード種類数 $f_\mathrm{c}$ の 3 つの適応度を定義した.
まず, $f_\mathrm{w}$ については, 表 \ref{jikken3env} に示される問題設定において, $f_\mathrm{w} = \exp(-\sum_{\mathrm{i}=0}^4 \sqrt{(0.50 - r_\mathrm{i})^2})$ と計算する.
また, $f_\mathrm{p}$, $f_\mathrm{c}$ に関しては, 表 \ref{table:OPdeck} のデッキと GA の個体が表す各カードのパラメータを比較してパラメータの変更量 $p$, 調整されたカードの種類数 $c$ を計算し, それぞれ $f_\mathrm{p} = \exp(-\frac{p}{200})$, $f_\mathrm{c} = \exp(-\frac{c}{15})$ と計算する.
表 \ref{table:gaparam} に本研究における GA のパラメータを示す.

     \section{結果と考察}
     以下, 各実験の結果とその考察を示す.
     
     \subsection{実験 1 : 結果と考察}
     表 \ref{table:winratejikken1} に学習後 10000 回対戦を実行し勝率を計算した結果を示す. この時, ベースラインとして,後攻に筆者らが実装した戦略 2 種類を基に行動するプレイヤー, 表 \ref{table:action} に沿ってランダムに行動するプレイヤーを配置し表 \ref{table:OPdeck} のデッキを使用して 10000 回ゲームを実行した際の勝率も示している. 
     \begin{table}[t]
       \centering
       \caption{後攻の戦略を変化させた場合の勝率比較}
       \vspace{-0.3cm}
       \label{table:winratejikken1}
       \scalebox{0.7}[0.7]{
         \begin{tabular}{|c|c|}
            \hline
            後攻の戦略        & 勝率     \\ \hline \hline
            学習済エージェント    & \textbf{0.7182} \\ \hline
            アグロ          & 0.6914 \\ \hline
            コントロール       & 0.6291 \\ \hline
            表 \ref{table:action} の行動空間に沿ってランダム & 0.2336       \\ \hline
            \end{tabular}
       }
       \end{table}
       これらのベースラインと比べて学習済エージェントは高い勝率を得ていることがわかる. DQN を用いて自動的にルールベースで作成した戦略よりも適した戦略を持つエージェントが構築できた.
       さらに学習済エージェントで 50000 回対戦を実行し, エージェントが構築した戦略を分析した.
       比較対象として, 本研究において $\epsilon$ は $\mathrm{\epsilon_{max}} = 1.0$ から指数関数的に減少するため, 表 \ref{table:winratejikken1} でベースラインとして用いた表 \ref{table:action} の行動空間に沿ってランダムに行動するエージェントを学習序盤のエージェントとして選んだ.\par 
       表 \ref{table:actioncount}に 50000 回の対戦において表 \ref{table:action} の行動空間における各行動でエージェントが選択した総数を示す. 大きな違いとしてランダムの場合はターンエンドが圧倒的に多く選ばれており, 学習済の場合は 2 番目に多い行動とほぼ同数になっていた点である. 表 \ref{table:action} の行動空間の定義ではエージェントが任意にターンエンドを選択できるようになっているが, 学習を進めていくにつれて無駄なターンエンドが減っていったと分かる.
       また. 学習済エージェントの行動に着目すると, 盤面のカードで相手プレイヤーに直接攻撃するアグロ寄りの戦略を構築していることがわかる. この理由としては, 表 \ref{table:OPdeck} の学習側に持たせるデッキにおいて強いカードとして恣意的に入れた ID 0 のカードは低コストで高い攻撃力を持つためと考えられる. \par
       また. 表 \ref{table:cardcount} に 50000 回の対戦において各エージェントが表 \ref{table:OPdeck} 内の各カードをプレイした回数を示す. 表 \ref{table:OPdeck} から, 両エージェント共にコストが小さいカードほど多くプレイされていることがわかる. ここで, 恣意的にカードを追加した ID 0, 13, 7 のコスト 1 帯のカードと, ID 14, 4 のコスト 5 帯のカードに注目すると, ランダムに行動するエージェントでは同コスト帯のカードの中でそこまで顕著な差は現れていない一方で, 学習済エージェントではコスト 1 帯のカードの中で ID 0 のプレイ回数が明らかに多くなっており, コスト 5 帯のカードの中でID 14 のカードは明らかに少なくなっていた. このことから学習の結果構築戦略下におけるカードの強弱も学習していると考えられ, 恣意的に入れたカードの強弱が結果に反映されており合理的であるといえる. 
       \begin{table}[t]
         \centering
         \caption{選択された総数が多い行動上位 5 個}
         \vspace{-0.3cm}
         \label{table:actioncount}
         \scalebox{0.65}[0.7]{
           \begin{tabular}{|cc|cc|}
             \hline
             \multicolumn{2}{|c|}{ランダム}      & \multicolumn{2}{c|}{学習済}       \\ \hline
             \multicolumn{1}{|c|}{行動説明} & 総数 & \multicolumn{1}{c|}{行動説明} & 総数 \\ \hline \hline
             \multicolumn{1}{|c|}{ターンエンド}    & 401838  & \multicolumn{1}{c|}{ターンエンド}    & 245646  \\ \hline
             \multicolumn{1}{|c|}{手札 4 を自盤面にプレイ}    &  92053  & \multicolumn{1}{c|}{手札 1 を自盤面にプレイ}    & 213804  \\ \hline
             \multicolumn{1}{|c|}{手札 1 を自盤面にプレイ}    & 63841  & \multicolumn{1}{c|}{自盤面 1 で相手プレイヤーに攻撃}    & 197221  \\ \hline
             \multicolumn{1}{|c|}{自盤面 1 で相手プレイヤーに攻撃}    & 63458  & \multicolumn{1}{c|}{自盤面 2 で相手プレイヤーに攻撃}    & 103490  \\ \hline
             \multicolumn{1}{|c|}{手札 2 を自盤面にプレイ}    & 61304  & \multicolumn{1}{c|}{手札 2 を自盤面にプレイ}    & 68018  \\ \hline
             \end{tabular}
         }
       \end{table}

       \begin{table}[t]
         \begin{minipage}{0.50\hsize}
           \centering
           \caption{各カードが盤面にプレイされた総数 (降順)}
            \vspace{-0.3cm}
            \label{table:cardcount}
           \scalebox{0.75}[0.75]{
           \begin{tabular}{|cc|cc|}
             \hline
             \multicolumn{2}{|c|}{ランダム}       & \multicolumn{2}{c|}{学習済}       \\ \hline
             \multicolumn{1}{|c|}{ID} & 総数    & \multicolumn{1}{c|}{ID} & 総数   \\ \hline \hline
             \multicolumn{1}{|c|}{13}  & 35173 & \multicolumn{1}{c|}{0}  & 32965 \\ \hline
             \multicolumn{1}{|c|}{0}  & 35163 & \multicolumn{1}{c|}{13} & 31845 \\ \hline
             \multicolumn{1}{|c|}{7}  & 35068 & \multicolumn{1}{c|}{7}  & 31581 \\ \hline
             \multicolumn{1}{|c|}{11} & 31134 & \multicolumn{1}{c|}{1}  & 26215 \\ \hline
             \multicolumn{1}{|c|}{9} & 30956 & \multicolumn{1}{c|}{11} & 26184 \\ \hline
             \multicolumn{1}{|c|}{1}  & 30672 & \multicolumn{1}{c|}{5}  & 25990 \\ \hline
             \multicolumn{1}{|c|}{5}  & 30401 & \multicolumn{1}{c|}{8}  & 25986 \\ \hline
             \multicolumn{1}{|c|}{8}  & 30386 & \multicolumn{1}{c|}{9}  & 25697 \\ \hline
             \multicolumn{1}{|c|}{12} & 26558 & \multicolumn{1}{c|}{12}  & 21763 \\ \hline
             \multicolumn{1}{|c|}{10} & 26521 & \multicolumn{1}{c|}{2}  & 21560 \\ \hline
             \multicolumn{1}{|c|}{6}  & 25866 & \multicolumn{1}{c|}{6} & 21393 \\ \hline
             \multicolumn{1}{|c|}{2}  & 25700 & \multicolumn{1}{c|}{10}  & 21367 \\ \hline
             \multicolumn{1}{|c|}{3} & 21301 & \multicolumn{1}{c|}{3}  & 19382 \\ \hline
             \multicolumn{1}{|c|}{14}  & 18751 & \multicolumn{1}{c|}{4} & 17639 \\ \hline
             \multicolumn{1}{|c|}{4}  & 18610 & \multicolumn{1}{c|}{14} & 16807 \\ \hline
             \end{tabular}
         }
         \end{minipage}%
         \begin{minipage}{0.50\hsize}
           \centering
           \caption{調整されるカード種類数を増やしながら単目的 GA を適応した結果}
            \label{jikken3result}
            \vspace{-0.3cm}
           \scalebox{0.70}[0.70]{
                   \begin{tabular}{|c|c|c|c|}
                     \hline
                     種類数     & $f_\mathrm{p}$ & $f_\mathrm{w}$ & $f_\mathrm{c}$\\ \hline \hline
                     1              & 0.9048         & 0.4805 & 0.9355  \\ \hline
                     2           & 0.8607         & 0.5893 & 0.8752 \\ \hline
                     3        & 0.8607         & 0.6131 & 0.8187  \\ \hline
                     4    & 0.7945         & 0.6339 & 0.7659 \\ \hline
                     5 & 0.7334         & 0.6395  & 0.7165 \\ \hline
                     6 & 0.6977      & 0.6729  & 0.6703 \\ \hline
                     7& 0.6637   & 0.7044  & 0.6271 \\ \hline
                     8 & 0.7047 & 0.6837 & 0.5866 \\ \hline
                     9 & 0.6505 & 0.7686 & 0.5488\\ \hline
                     10 & 0.6250 & 0.7728  & 0.5134\\ \hline
                     11 & 0.5169 & 0.7627 & 0.4803\\ \hline
                     12 & 0.5326 & 0.8078 & 0.4493\\ \hline
                     13 & 0.6005 & 0.8103 & 0.4204\\ \hline
                     14 &  0.5220 &  0.8261 & 0.3932\\ \hline
                     \end{tabular}
                 }
         \end{minipage}
       \end{table}
     
         \subsection{実験 2 : 結果と考察}
         実験 2 では, 同戦略同士が表 \ref{table:OPdeck} を用いてカードをそれぞれ 1 種類ずつ除いた際の先攻側の勝率を計算した. よって, 計 $15^2 = 225$ 個の勝率の値が結果として得られる. この 225 個の勝率の値の中で最大値, 最小値に着目すると戦略下における最もカードパワーが強いカード, 弱いカードを判断できる.
         \par
         アグロ同士の対戦では最小値は先攻から ID 0 を除いて後攻から ID 14 を除いた際の先攻の勝率の値となった. よってアグロの戦略下においては, デッキ内で ID 0 のカードが最も強いカード, ID 14 のカードが最も弱いカードと判断できる. 
         \par
         同様に,  学習済エージェント同士の対戦から得られた結果と比較すると, どちらの戦略同士の対戦においても ID 0 のカードが最も強いカードと判断された一方で, 最も弱いカードに関しては学習済エージェント同士の対戦結果では ID 8, アグロ同士の対戦結果では ID 14 となり, 戦略によって異なる結果となった. これは実験 1 の事前学習の効果により, 学習済エージェント同士の対戦において恣意的に弱く設定したカードは登場回数が少なく勝率計算に及ぼす影響が小さかったためと考えられる. 
      
               \subsection{実験 3 : 結果と考察}
               提案手法 3 の数値実験の際にはパラメータを調整するカードの優先度を決定する必要がある.
               本研究では, 以下の手順で調整すべきカードの優先順位を決定した. 
               \begin{enumerate}
                  \small
                  \setlength{\itemsep}{0cm} % 項目間
                 \item 得られた勝率の値の最大値,最小値を見て, 最も強いカード, 最も弱いカードを確かめる
                 \item その 2 枚において先攻後攻そのカードを除いた時の勝率を見て, カードを除いていない時の勝率と比較し, 差の絶対値が大きいカードを変更する優先度が最高のカードとする
                 \item 先攻で そのカードを除いた状態で後攻で何も除いていない場合の勝率を計算する.
                 \item 先攻が優先度最高のカードを除いた場合において, 3 で計算した勝率と差の絶対値を取る. 
                 \item 値が大きいほど変更する優先度を高く設定する.
               \end{enumerate}
               このように決定した優先順位は表 \ref{table:OPdeck} におけるカード ID で表すと, 
               $0 > 8 > 6 > 10 > 4 > 1 > 3 > 14 > 2 > 11 > 13 > 9 > 7 > 12 > 5$ となった. \par
               表 \ref{jikken3result} に, この優先順位に沿って調整するカードの種類数を増やしていった場合の結果を示す.
               調整するカードの種類, すなわち GA の解空間の次元が増えるほど $f_\mathrm{w}$ に関して良い値を持つ解が得られていた. 
                 \begin{table}[t]
                  \centering
                  \caption{各手法で得られた最も良好な解の適応度}
                  \label{res_3}
                  \vspace{-0.3cm}
                  
                  \scalebox{0.75}[0.75]{
                    \begin{tabular}{|c|c|c|c|}
                      \hline
                      手法        & $f_\mathrm{p}$ & $f_\mathrm{w}$ & $f_\mathrm{c}$ \\ \hline \hline
                      単目的 GA      & 0.44933         & \textbf{0.85146}   & 0.36788          \\ \hline
                      多目的 GA  & \textbf{0.66365}         & 0.79097   & 0.42035          \\ \hline
                      提案手法   & 0.53259              &  0.80783     & \textbf{0.44933}  \\ \hline
                      \end{tabular}
                  }
                  \vspace{-0.3cm}
                  \end{table}
   比較手法として用いたデッキ内のすべてのカードを対象とした単目的 GA, 多目的 GA に関してそれぞれ最も良好な解を最終世代の適応度が最も大きい解, 最終世代のパレートフロント上で目的関数 $f_\mathrm{w}$ の値が最も大きい解とすると, 表 \ref{jikken3result} 内の解で $f_\mathrm{w}$ に関して 2 つの比較手法の最も良好な解と中間的な解が複数得られた. その解の中で $f_\mathrm{c}$ の値が最も大きい解を提案手法において最も良好な解とした.\par
   表 \ref{res_3} に各手法で得られた最も良好な解の適応度を示す.
各手法において, それぞれの適応度に関して優越した解が得られていた. また, 提案手法における最も良好な解は$f_\mathrm{w}$ に関して多目的 GA により得られた解に優越しており, $f_\mathrm{c}$ に関しては他の手法により得られた解に関して優越していた. よって調整されるカードの枚数を最小限にする TCG 環境のゲームバランス調整という提案手法 3 の有効性が確かめられた. 

\section{まとめと今後の課題}
本研究では, 深層強化学習を用いた TCG 環境の最適化手法を提案し, 数値実験により有効性を示した. 今後の課題として, デッキ間の相性を考慮したメタゲームの概念を取り入れた TCG 環境のゲームバランス最適化の検討, 不完全情報ゲームへの適用で顕著な成果を残している ReBeL \cite{ReBeL} によるエージェントの構築などが挙げられる. 
%index.bibはtexファイルと同階層に置く
%ちゃんと\citeしないと表示されない(1敗)
\bibliography{index.bib}
\bibliographystyle{junsrt}

\end{document}