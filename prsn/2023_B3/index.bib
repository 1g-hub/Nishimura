@ARTICLE{DQN,
       author = {{Wang}, Ziyu and {Schaul}, Tom and {Hessel}, Matteo and {van Hasselt}, Hado and {Lanctot}, Marc and {de Freitas}, Nando},
        title = "{Dueling Network Architectures for Deep Reinforcement Learning}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning},
         year = 2015,
        month = nov,
          eid = {arXiv:1511.06581},
        pages = {arXiv:1511.06581},
archivePrefix = {arXiv},
       eprint = {1511.06581},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2015arXiv151106581W},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{AlphaZero,
       author = {{Silver}, David and {Hubert}, Thomas and {Schrittwieser}, Julian and {Antonoglou}, Ioannis and {Lai}, Matthew and {Guez}, Arthur and {Lanctot}, Marc and {Sifre}, Laurent and {Kumaran}, Dharshan and {Graepel}, Thore and {Lillicrap}, Timothy and {Simonyan}, Karen and {Hassabis}, Demis},
        title = "{Mastering Chess and Shogi by Self-Play with a General Reinforcement Learning Algorithm}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
         year = 2017,
        month = dec,
          eid = {arXiv:1712.01815},
        pages = {arXiv:1712.01815},
archivePrefix = {arXiv},
       eprint = {1712.01815},
 primaryClass = {cs.AI},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2017arXiv171201815S},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@ARTICLE{OpenAIGym,
       author = {{Brockman}, Greg and {Cheung}, Vicki and {Pettersson}, Ludwig and {Schneider}, Jonas and {Schulman}, John and {Tang}, Jie and {Zaremba}, Wojciech},
        title = "{OpenAI Gym}",
      journal = {arXiv e-prints},
     keywords = {Computer Science - Machine Learning, Computer Science - Artificial Intelligence},
         year = 2016,
        month = jun,
          eid = {arXiv:1606.01540},
        pages = {arXiv:1606.01540},
archivePrefix = {arXiv},
       eprint = {1606.01540},
 primaryClass = {cs.LG},
       adsurl = {https://ui.adsabs.harvard.edu/abs/2016arXiv160601540B},
      adsnote = {Provided by the SAO/NASA Astrophysics Data System}
}
@article{mnih2013atari,
  abstract = {We present the first deep learning model to successfully learn control
policies directly from high-dimensional sensory input using reinforcement
learning. The model is a convolutional neural network, trained with a variant
of Q-learning, whose input is raw pixels and whose output is a value function
estimating future rewards. We apply our method to seven Atari 2600 games from
the Arcade Learning Environment, with no adjustment of the architecture or
learning algorithm. We find that it outperforms all previous approaches on six
of the games and surpasses a human expert on three of them.},
  added-at = {2019-07-12T20:11:01.000+0200},
  author = {Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Graves, Alex and Antonoglou, Ioannis and Wierstra, Daan and Riedmiller, Martin},
  biburl = {https://www.bibsonomy.org/bibtex/2a00ec4c09f5dc9b3f8a1836f4e02bb5d/lanteunis},
  interhash = {78966703f649bae69a08a6a23a4e8879},
  intrahash = {a00ec4c09f5dc9b3f8a1836f4e02bb5d},
  keywords = {},
  note = {cite arxiv:1312.5602Comment: NIPS Deep Learning Workshop 2013},
  timestamp = {2019-07-12T20:11:01.000+0200},
  title = {Playing Atari with Deep Reinforcement Learning},
  url = {http://arxiv.org/abs/1312.5602},
  year = 2013
}
