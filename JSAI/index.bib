@article{DQN,
  author    = {Volodymyr Mnih and
               Koray Kavukcuoglu and
               David Silver and
               Alex Graves and
               Ioannis Antonoglou and
               Daan Wierstra and
               Martin A. Riedmiller},
  title     = {Playing Atari with Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1312.5602},
  year      = {2013},
  url       = {http://arxiv.org/abs/1312.5602},
  eprinttype = {arXiv},
  eprint    = {1312.5602},
  timestamp = {Mon, 13 Aug 2018 16:47:42 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/MnihKSGAWR13.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Hearthstone,
author = {Mesentier Silva, Fernando de and Canaan, Rodrigo and Lee, Scott and Fontaine, Matthew C. and Togelius, Julian and Hoover, Amy K.},
title = {Evolving the Hearthstone Meta},
year = {2019},
publisher = {IEEE Press},
url = {https://doi.org/10.1109/CIG.2019.8847966},
doi = {10.1109/CIG.2019.8847966},
abstract = {Balancing an ever growing strategic game of high complexity, such as Hearthstone is a complex task. The target of making strategies diverse and customizable results in a delicate intricate system. Tuning over 2000 cards to generate the desired outcome without disrupting the existing environment becomes a laborious challenge. In this paper, we discuss the impacts that changes to existing cards can have on strategy in Hearthstone. By analyzing the win rate on match-ups across different decks, being played by different strategies, we propose to compare their performance before and after changes are made to improve or worsen different cards. Then, using an evolutionary algorithm, we search for a combination of changes to the card attributes that cause the decks to approach equal, 50% win rates. We then expand our evolutionary algorithm to a multi-objective solution to search for this result, while making the minimum amount of changes, and as a consequence disruption, to the existing cards. Lastly, we propose and evaluate metrics to serve as heuristics with which to decide which cards to target with balance changes.},
booktitle = {2019 IEEE Conference on Games (CoG)},
pages = {1â€“8},
numpages = {8},
location = {London, United Kingdom}
}
@ARTICLE{NSGA-2,
  author={Deb, K. and Pratap, A. and Agarwal, S. and Meyarivan, T.},
  journal={IEEE Transactions on Evolutionary Computation}, 
  title={A fast and elitist multiobjective genetic algorithm: NSGA-II}, 
  year={2002},
  volume={6},
  number={2},
  pages={182-197},
  doi={10.1109/4235.996017}
}
@inproceedings{ReBeL,
author = {Brown, Noam and Bakhtin, Anton and Lerer, Adam and Gong, Qucheng},
title = {Combining Deep Reinforcement Learning and Search for Imperfect-Information Games},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {The combination of deep reinforcement learning and search at both training and test time is a powerful paradigm that has led to a number of successes in single-agent settings and perfect-information games, best exemplified by AlphaZero. However, prior algorithms of this form cannot cope with imperfect-information games. This paper presents ReBeL, a general framework for self-play reinforcement learning and search that provably converges to a Nash equilibrium in any two-player zero-sum game. In the simpler setting of perfect-information games, ReBeL reduces to an algorithm similar to AlphaZero. Results in two different imperfect-information games show ReBeL converges to an approximate Nash equilibrium. We also show ReBeL achieves superhuman performance in heads-up no-limit Texas hold'em poker, while using far less domain knowledge than any prior poker AI.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {1431},
numpages = {13},
location = {Vancouver, BC, Canada},
series = {NIPS'20}
}

